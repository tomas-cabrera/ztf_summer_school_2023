{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning for Time Domain Astronomy\n",
    "ZTF Summer School 2023\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirements:\n",
    "- Matrix Convolution/Kernel operations: https://en.wikipedia.org/wiki/Kernel_(image_processing)\n",
    "- Neural Networks/Perceptron: http://neuralnetworksanddeeplearning.com/chap1.html#perceptrons\n",
    "- Convolutional Neural Networks (CNN): http://neuralnetworksanddeeplearning.com/chap6.html#introducing_convolutional_networks\n",
    "- Tensorflow and Keras, which we'll use to create the models and train them: https://www.tensorflow.org/tutorials/keras/classification\n",
    "- ZTF alerts: https://www.ztf.caltech.edu/ztf-alert-stream.html and https://iopscience.iop.org/article/10.1088/1538-3873/aae904/pdf\n",
    "\n",
    "Optional, but recommended:\n",
    "- Weights and Biases Quickstart: https://docs.wandb.ai/quickstart (you will need to create an account anyway!)\n",
    "- Early stopping: https://en.wikipedia.org/wiki/Early_stopping\n",
    "- Dropout: https://en.wikipedia.org/wiki/Dilution_(neural_networks)\n",
    "- Plateau phenomenon: https://analyticsindiamag.com/what-is-the-plateau-problem-in-neural-networks-and-how-to-fix-it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview:\n",
    "\n",
    "We'll have a look at how the alert scanning process can be automatized (or at least facilitated) by teaching neural networks how to look at the cutouts and metadata from the alert packets just like humans would. We'll talk about the model's architecture, how to train it, evaluate it, and how to fine-tune it with the help of wandb.ai.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# but of course, let's start with the imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import wandb\n",
    "from ast import literal_eval\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, Concatenate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading the dataset (run only once):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=16ufsYYbpJ0ZWw2ZIibPfQQ81k7OBNl9Z' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=16ufsYYbpJ0ZWw2ZIibPfQQ81k7OBNl9Z\" -O data.zip && rm -rf /tmp/cookies.txt\n",
    "# !unzip data.zip -d data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick reminder: Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"In deep learning, a convolutional neural network (CNN) is a class of artificial neural network most commonly applied to analyze visual imagery.[1] CNNs use a mathematical operation called convolution in place of general matrix multiplication in at least one of their layers.[2] They are specifically designed to process pixel data and are used in image recognition and processing.\"\n",
    "*https://en.wikipedia.org/wiki/Convolutional_neural_network*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/v2/resize:fit:1162/format:webp/1*tvwYybdIwvoOs0DuUEJJTg.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "<p style=\"font-size: 10px;\">Source: https://medium.com/techiepedia/binary-image-classifier-cnn-using-tensorflow-a3f5d6746697</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A basic CNN is composed of several layers:\n",
    "- Convolutional layers: they apply a convolution operation to the input, passing the result to the next layer.\n",
    "- Pooling layers: they downsample the input along the spatial dimensions (width, height), reducing the number of parameters and computation in the network.\n",
    "- Fully connected layers: they compute the class scores, resulting in a vector of size equal to the number of classes. If the classification we expect is binary (simply True if >0.5, otherwise False), the last layer is a single neuron with a sigmoid activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input can be either a single image, composed of one or more channels (e.g. RGB), or a batch of images of the same size. When it comes to the cutouts found in the ZTF alert packets, they are a set of 3 images (science, template, difference) of size 63x63 pixels, each with 1 channel (grayscale). Meaning the imput is of size (63, 63, 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./alert_cutouts.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "<p style=\"font-size: 10px;\">Screenshot (from fritz.science) of the cutouts from the latest alert for ZTF23aaqqfac</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about metadata?\n",
    "\n",
    "Often, the data available isn't just one or more image, but also metadata that was computed which can be used to improve the classification. For example, the metadata can be the position of the object, the time of the observation, the magnitude of the object, etc. The metadata can be used in two ways:\n",
    "- As an additional input to the CNN, concatenated to the image.\n",
    "- As an additional input to the CNN, concatenated to the output of the last convolutional layer.\n",
    "\n",
    "Later in this notebook, you'll see that the process of \"merging\" the CNN model and the metadata (which is a classic neural network) model into one is extremely simple.\n",
    "\n",
    "The metadata goes through its own set of fully connected layers, and the output is concatenated to the output of the last convolutional layer after it has been flattened. The resulting vector is then fed to the fully connected layers that later compute the class scores. The BTS model aims to predict one thing: the probability that the object is a bright transient, of interest for the Bright Transient Survey (BTS) group.\n",
    "\n",
    "<img src=\"http://neuralnetworksanddeeplearning.com/images/tikz11.png\" alt=\"Drawing\" style=\"width: 800px; background-color: white;\"/>\n",
    "<p style=\"font-size: 10px;\">Basic neural network with 2 hidden layers, with one neuron as an ouput to perform binary classification. Just like the one our alert metadata will go through. Source: http://neuralnetworksanddeeplearning.com/chap1.html</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But why do we need AI for any of this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa327668d-b172-44e7-a2b0-4671c235ce79_682x500.jpeg\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The Bright Transient Survey (BTS) relies on visual inspection (“scanning”) to select sources\n",
    "for accomplishing its mission of spectroscopically classifying all bright extragalactic transients\n",
    "found by the Zwicky Transient Facility (ZTF).\n",
    "We present BTSbot, a multi-input convolutional\n",
    "neural network, which provides a bright transient\n",
    "score to individual ZTF detections using their image data and 14 extracted features. BTSbot eliminates the need for scanning by automatically identifying and requesting follow-up observations of\n",
    "new bright (m < 18.5 mag) transient candidates.\n",
    "In validation, BTSbot outperforms BTS scanners in terms of completeness (99% vs. 95%) and\n",
    "identification speed (on average, 7.4 h quicker).\"\n",
    "\n",
    "Source: Nabeel :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize: Instead of relaying on people to look at thousands of alerts every day to find which one are worth getting a spectra for, we train a model to do it for us. Just like a person scanning, the model can look at both images and metadata, except more consistently and orders of magnitude faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BTS model\n",
    "\n",
    "As mentioned earlier, the model is composed of two parts: the CNN and the metadata part, merged to yield one output. Here is what it looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN model:\n",
    "- 2 blocks of 2 convolutional layers, each followed by a pooling layer, and a dropout layer.\n",
    "- a flatten layer.\n",
    "\n",
    "Metadata model:\n",
    "- 2 fully connected layers.\n",
    "\n",
    "Merged model:\n",
    "- the output of the flatten layer of the CNN model is concatenated to the output of the metadata model.\n",
    "- Followed by one fully connected layer, a dropout layer, and the output layer (a single neuron with a sigmoid activation function).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define some basic constants.\n",
    "config = {\n",
    "    'image_size': (63,63,3),\n",
    "    'metadata_size': (14,),\n",
    "    'dropout_1': 0.3,\n",
    "    'dropout_2': 0.3,\n",
    "    'dropout_3': 0.3,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we prepare the input layers\n",
    "triplet_input = keras.Input(shape=config[\"image_size\"], name='triplet')\n",
    "meta_input = keras.Input(shape=config[\"metadata_size\"], name='metadata')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN part of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first conv block\n",
    "x_conv = Conv2D(16, (3, 3), activation='relu', padding='same', input_shape=(63, 63, 3), name='conv1')(triplet_input)\n",
    "x_conv = Conv2D(16, (3, 3), activation='relu', padding='same', name='conv2')(x_conv)\n",
    "x_conv = MaxPooling2D(pool_size=(2, 2), name='pool1')(x_conv)\n",
    "x_conv = Dropout(config[\"dropout_1\"], name='drop1')(x_conv)\n",
    "\n",
    "# second conv block\n",
    "x_conv = Conv2D(32, (3, 3), activation='relu', padding='same', name='conv3')(x_conv)\n",
    "x_conv = Conv2D(32, (3, 3), activation='relu', padding='same', name='conv4')(x_conv)\n",
    "x_conv = MaxPooling2D(pool_size=(4, 4), name='pool2')(x_conv)\n",
    "x_conv = Dropout(config[\"dropout_2\"], name='drop2')(x_conv)\n",
    "\n",
    "# we flatten the output\n",
    "x_conv = Flatten()(x_conv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metadata part of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata model\n",
    "x_meta = Dense(16, activation='relu', name='metadata_fc_1')(meta_input)\n",
    "x_meta = Dense(32, activation='relu', name='metadata_fc_2')(x_meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merged model\n",
    "x = Concatenate(axis=1)([x_conv, x_meta])\n",
    "x = Dense(16, activation='relu', name='comb_fc_2')(x)\n",
    "x = Dropout(config[\"dropout_3\"])(x)\n",
    "\n",
    "# Output (binary classification, a single sigmoid neuron giving us a score between 0 (not BTS) and 1 (BTS!!!))\n",
    "output = Dense(1, activation='sigmoid', name='fc_out')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compile the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Model(inputs=[triplet_input, meta_input], outputs=output, name=\"mi_cnn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the model with a graph:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./model.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "Easy, right? :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just test it on a fake input, to verify that the model is working as expected (all the layers are connected correctly, the inputs and outputs has the right shape, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 59ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.5]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_triplet = np.zeros((1, 63, 63, 3))\n",
    "fake_meta = np.zeros((1, 14))\n",
    "\n",
    "model.predict([fake_triplet, fake_meta])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just refactor all this to have a nice method to instantiate a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BTSModel(config):\n",
    "    #image size and metadata size might be strings, so we convert them to tuples with a literal eval\n",
    "    if type(config[\"image_size\"]) == str:\n",
    "        image_size = literal_eval(config[\"image_size\"])\n",
    "    else:\n",
    "        image_size = config[\"image_size\"]\n",
    "    if type(config[\"metadata_size\"]) == str:\n",
    "        metadata_size = literal_eval(config[\"metadata_size\"])\n",
    "    else:\n",
    "        metadata_size = config[\"metadata_size\"]\n",
    "\n",
    "    # first we prepare the input layers\n",
    "    triplet_input = keras.Input(shape=image_size, name='triplet')\n",
    "    meta_input = keras.Input(shape=metadata_size, name='metadata')\n",
    "\n",
    "    # first conv block\n",
    "    x_conv = Conv2D(16, (3, 3), activation='relu', padding='same', input_shape=(63, 63, 3), name='conv1')(triplet_input)\n",
    "    x_conv = Conv2D(16, (3, 3), activation='relu', padding='same', name='conv2')(x_conv)\n",
    "    x_conv = MaxPooling2D(pool_size=(2, 2), name='pool1')(x_conv)\n",
    "    x_conv = Dropout(config[\"dropout_1\"], name='drop1')(x_conv)\n",
    "\n",
    "    # second conv block\n",
    "    x_conv = Conv2D(32, (3, 3), activation='relu', padding='same', name='conv3')(x_conv)\n",
    "    x_conv = Conv2D(32, (3, 3), activation='relu', padding='same', name='conv4')(x_conv)\n",
    "    x_conv = MaxPooling2D(pool_size=(4, 4), name='pool2')(x_conv)\n",
    "    x_conv = Dropout(config[\"dropout_2\"], name='drop2')(x_conv)\n",
    "\n",
    "    # we flatten the output\n",
    "    x_conv = Flatten()(x_conv)\n",
    "\n",
    "    # Metadata model\n",
    "    x_meta = Dense(16, activation='relu', name='metadata_fc_1')(meta_input)\n",
    "    x_meta = Dense(32, activation='relu', name='metadata_fc_2')(x_meta)\n",
    "\n",
    "    # Merged model\n",
    "    x = Concatenate(axis=1)([x_conv, x_meta])\n",
    "    x = Dense(16, activation='relu', name='comb_fc_2')(x)\n",
    "    x = Dropout(config[\"dropout_3\"])(x)\n",
    "\n",
    "    # Output (binary classification, a single sigmoid neuron giving us a score between 0 (not BTS) and 1 (BTS!!!))\n",
    "    output = Dense(1, activation='sigmoid', name='fc_out')(x)\n",
    "\n",
    "    model = keras.Model(inputs=[triplet_input, meta_input], outputs=output, name=\"mi_cnn\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we have a model that works. But just like a baby's brain (but even worse), this thing can't do anything! It needs to be trained through trial and error, just like the scanners would. Let's spoon feed it with a ton of data so it can steal our jobs faster :)\n",
    "\n",
    "<img src=\"https://styles.redditmedia.com/t5_adbcw/styles/communityIcon_anc30b6ykk461.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "<p style=\"font-size: 10px;\">Source: https://www.reddit.com/r/machinelearningmemes</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "We'll start by creating a training loop, which will be used to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we add to our config some of the constants used in the training process\n",
    "config.update({\n",
    "    'batch_size': 32,\n",
    "    'epochs': 10,\n",
    "    'learning_rate': 0.001,\n",
    "    'loss': 'binary_crossentropy',\n",
    "    'optimizer': 'adam',\n",
    "    'random_seed': 42,\n",
    "    'train_data_version': \"ZTFSS\",\n",
    "    'early_stopping_patience': 10,\n",
    "    'LR_plateau_patience': 20,\n",
    "    'reduce_LR_factor': 0.5,\n",
    "    'reduce_LR_minLR': 1e-6, \n",
    "    'beta_1': 0.9,\n",
    "    'beta_2': 0.999,\n",
    "    'metadata_cols': [\n",
    "      \"sgscore1\",\n",
    "      \"distpsnr1\",\n",
    "      \"sgscore2\",\n",
    "      \"distpsnr2\",\n",
    "      \"fwhm\",\n",
    "      \"magpsf\",\n",
    "      \"sigmapsf\",\n",
    "      \"ra\",\n",
    "      \"dec\",\n",
    "      \"diffmaglim\",\n",
    "      \"ndethist\",\n",
    "      \"nmtchps\",\n",
    "      \"age\",\n",
    "      \"peakmag_so_far\"\n",
    "    ],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.keras.utils.set_random_seed(config[\"random_seed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### => TEMPORARY SECTION, TO REPLACE ONCE WE HAVE THE DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading the data:\n",
    "For this session, the dataset is already prepared and stored for you to use. It is composed of N alerts, each with 3 cutouts (science, template, difference) and 14 metadata features (2 of these features have been computed by us based on the alert history for a given object, and do not come with the basic alert packet). The training set is used to train the model, the validation set is used to evaluate the model during training.\n",
    "\n",
    "Later (after the training), we'll use the test set to evaluate the model on data it has never seen before. This is important to make sure the model is not overfitting the training data, and that it can generalize to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cand = pd.read_csv(f'data/train_cand_{config[\"train_data_version\"]}.csv')\n",
    "triplets = np.load(f'data/train_triplets_{config[\"train_data_version\"]}.npy', mmap_mode='r')\n",
    "val_cand = pd.read_csv(f'data/val_cand_{config[\"train_data_version\"]}.csv')\n",
    "val_triplets = np.load(f'data/val_triplets_{config[\"train_data_version\"]}.npy', mmap_mode='r')\n",
    "\n",
    "gen_cols = np.append(config['metadata_cols'], ['label'])\n",
    "\n",
    "x_train, y_train = triplets, cand['label']\n",
    "x_val, y_val = val_triplets, val_cand['label']\n",
    "\n",
    "# train_df is a combination of the desired metadata cols and y_train (labels)\n",
    "# we provide the model a custom generator function to separate these as necessary\n",
    "train_df = cand[gen_cols]\n",
    "val_df = val_cand[gen_cols]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we prepare the data a little further for the training loop. We'll use the tf.keras.preprocessing.image.ImageDataGenerator to perform some data augmentation, which will help the model generalize better.\n",
    "\n",
    "Also, we will weight the classes to account for the class imbalance. The weight of each class is the inverse of the class frequency in the training set, in case we have more BTS than non-BTS objects, or the opposite. This will help the model learn to classify both classes equally well, and reduce the false positives and false negatives rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator()\n",
    "val_datagen = tf.keras.preprocessing.image.ImageDataGenerator()\n",
    "\n",
    "t_generator = train_datagen.flow(x_train, train_df, batch_size=config[\"batch_size\"], seed=config[\"random_seed\"], shuffle=False)\n",
    "v_generator = val_datagen.flow(x_val, val_df, batch_size=config[\"batch_size\"], seed=config[\"random_seed\"], shuffle=False)\n",
    "\n",
    "def multiinput_train_generator():\n",
    "    while True:\n",
    "        # get the data from the generator\n",
    "        # data is [[img], [metadata and labels]]\n",
    "        # yields batch_size number of entries\n",
    "        data = t_generator.next()\n",
    "\n",
    "        imgs = data[0]\n",
    "        cols = data[1][:,:-1]\n",
    "        targets = data[1][:,-1:]\n",
    "\n",
    "        yield [imgs, cols], targets\n",
    "\n",
    "def multiinput_val_generator():\n",
    "    while True:\n",
    "        data = v_generator.next()\n",
    "\n",
    "        imgs = data[0]\n",
    "        cols = data[1][:,:-1]\n",
    "        targets = data[1][:,-1:]\n",
    "\n",
    "        yield [imgs, cols], targets\n",
    "\n",
    "training_generator = multiinput_train_generator()\n",
    "validation_generator = multiinput_val_generator()\n",
    "\n",
    "# weight data on number of ALERTS per class\n",
    "num_training_examples_per_class = np.array([np.sum(cand['label'] == 0), np.sum(cand['label'] == 1)])\n",
    "\n",
    "# fewer examples -> larger weight\n",
    "weights = (1 / num_training_examples_per_class) / np.linalg.norm((1 / num_training_examples_per_class))\n",
    "normalized_weight = weights / np.max(weights)\n",
    "\n",
    "class_weight = {i: w for i, w in enumerate(normalized_weight)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of training our model for a fixed number of epochs, we'll use early stopping. This means that the model will train until it stops improving, and then stop. This is done by monitoring the validation loss, and stopping the training if it doesn't improve for a set number of epochs (patience).\n",
    "Also, to keep training further once we plateau, we'll use a learning rate scheduler. This will reduce the learning rate by a factor of N (dividing it) once the validation loss stops improving for a set number of epochs (patience).\n",
    "\n",
    "You might have seen in the config earlier that we still specify a number of epochs. You can see this as a maximum number of epochs, which will be reached if the model doesn't stop training before with early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we set some rules to stop the training process early if the validation loss does not improve\n",
    "# halt training if no improvement in validation loss over patience epochs\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    verbose=1, \n",
    "    patience=config['early_stopping_patience']\n",
    ")\n",
    "\n",
    "# reduce learning rate if no improvement in validation loss over patience epochs\n",
    "LR_plateau = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\", \n",
    "    patience=config['LR_plateau_patience'],\n",
    "    factor=config['reduce_LR_factor'],\n",
    "    min_lr=config['reduce_LR_minLR'],\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use [Weights and Biases](https://wandb.ai/) to log the training metrics, and to save the model. You can create a free account on https://wandb.ai/ to log your own metrics. Later in this notebook, we'll make even better use of wandb.ai to fine-tune the model with the *sweeps* feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"BTSbot\")\n",
    "# Send parameters of this run to WandB\n",
    "for param in list(config):\n",
    "    wandb.config[param] = config[param]\n",
    "\n",
    "run_name = wandb.run.name\n",
    "WandBLogger = wandb.keras.WandbMetricsLogger(log_freq=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use an Adam optimizer, which is a popular optimizer for deep learning. It is an adaptive learning rate optimization algorithm that's been designed specifically for training deep neural networks. It's a little more complex than the classic Stochoastic Gradient Descent (SGD) optimizer, but it's more efficient and requires less manual tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=config['learning_rate'], \n",
    "    beta_1=config['beta_1'],\n",
    "    beta_2=config['beta_2']\n",
    ")\n",
    "model.compile(optimizer=optimizer, loss=config['loss'], metrics=['accuracy'])\n",
    "\n",
    "#PS: If you are using tensorflow v2.11+ on M1/M2/M3 macs, you might benefit from using\n",
    "# the legacy version of the optimizer, which is: tf.keras.optimizers.legacy.Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now (finally!), let's train the model!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "258/258 [==============================] - 13s 48ms/step - loss: 0.9511 - accuracy: 0.6624 - val_loss: 0.6433 - val_accuracy: 0.6816 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "258/258 [==============================] - 13s 50ms/step - loss: 0.5773 - accuracy: 0.7140 - val_loss: 0.5915 - val_accuracy: 0.7023 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "258/258 [==============================] - 13s 52ms/step - loss: 0.5659 - accuracy: 0.7137 - val_loss: 0.6584 - val_accuracy: 0.6484 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "258/258 [==============================] - 13s 52ms/step - loss: 0.5540 - accuracy: 0.7241 - val_loss: 0.6014 - val_accuracy: 0.6965 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "258/258 [==============================] - 13s 51ms/step - loss: 0.5433 - accuracy: 0.7288 - val_loss: 0.5865 - val_accuracy: 0.6943 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "258/258 [==============================] - 13s 52ms/step - loss: 0.5433 - accuracy: 0.7329 - val_loss: 0.6114 - val_accuracy: 0.6988 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "258/258 [==============================] - 13s 52ms/step - loss: 0.5348 - accuracy: 0.7323 - val_loss: 0.6209 - val_accuracy: 0.6685 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "258/258 [==============================] - 13s 52ms/step - loss: 0.5362 - accuracy: 0.7349 - val_loss: 0.6014 - val_accuracy: 0.6768 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "258/258 [==============================] - 13s 51ms/step - loss: 0.5310 - accuracy: 0.7338 - val_loss: 0.6058 - val_accuracy: 0.6979 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "258/258 [==============================] - 14s 53ms/step - loss: 0.5196 - accuracy: 0.7438 - val_loss: 0.5617 - val_accuracy: 0.7271 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "h = model.fit(\n",
    "    training_generator,\n",
    "    steps_per_epoch=0.8*len(x_train) // config[\"batch_size\"],\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=(0.8*len(x_val)) // config[\"batch_size\"],\n",
    "    epochs=config[\"epochs\"],\n",
    "    verbose=1, callbacks=[early_stopping, LR_plateau, WandBLogger]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Testing-the-model\"></a>\n",
    "### Testing the model\n",
    "\n",
    "Now, we'll load some test data to evaluate the model on data it has never seen before. This is important to make sure the model is not overfitting the training data, and that it can generalize to new data. Also, it will set a baseline for us to compare the model we'll fine-tune later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "324/324 [==============================] - 3s 8ms/step\n",
      "Overall validation accuracy 77.81%\n"
     ]
    }
   ],
   "source": [
    "raw_preds = model.predict([triplets, cand.loc[:,config[\"metadata_cols\"]]], batch_size=config['batch_size'], verbose=1)\n",
    "preds = np.rint(np.transpose(raw_preds))[0].astype(int)\n",
    "labels = cand[\"label\"].to_numpy(dtype=int)\n",
    "\n",
    "results = preds == labels\n",
    "print(f\"Overall validation accuracy {100*np.sum(results) / len(results):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the accuracy isn't too bad, and it looks like the model already learned from the data with the hyperparameters we chose. But we can do better!\n",
    "\n",
    "**However**, accuracy isn't the only metric we should look at. There're a lot more metrics that can be used to evaluate a model, and it's important to look at them all to get a better idea of how the model is performing. For example, the accuracy can be misleading if the dataset is imbalanced (which is the case here). If we have 99% of non-BTS objects, and 1% of BTS objects, a model that always predicts non-BTS will have an accuracy of 99%, but it's not a good model. This is why we'll look can look at the confusion matrix, the precision, the recall, and the F1 score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here is are your first set of tasks:**\n",
    "\n",
    "1. Using matplotlib or your favorite plotting library, plot the confusion matrix for the test set. Hint: You can use the [sklearn.metrics.confusion_matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) function to compute the confusion matrix, and the [sklearn.metrics.ConfusionMatrixDisplay](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html) to plot it.\n",
    "2. Calculate the precision, recall, and F1 score for the test set. Hint: sklearn has some tools to help you with that :)\n",
    "3. Nowadays, it's more common to also look at a Receiver Operating Characteristic (ROC) curve and AUC: https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc. Plot the ROC curve and compute the AUC for the test set.\n",
    "\n",
    "For each of the plots and metrics you come up with, try your best to interpret the results. What do they mean? What do they tell you about the model? What can you conclude from them?\n",
    "\n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization: Grid search using Weights and Biases Sweeps\n",
    "\n",
    "After looking at our basic model's result, we can easily imagine how it can be improved. One way to do that is to try different values for all of the hyperparemeters it uses, hoping to fine-tune their value to get the best result our of our existing model. This is called hyperparameter optimization, and it's a very common practice in machine learning. A great tool to do this automatically is a grid search, which will try all the possible combinations of hyperparameters you want to try, and return the best one. However, this can be very time consuming, and it's not always possible to try all the possible combinations. This is where Weights and Biases Sweeps comes in handy.\n",
    "\n",
    "Let's use wandb.ai to fine-tune the model with their *sweeps* feature, which works pretty much like a grid search, but smarter. We'll use Weights and Biases Sweeps to perform a grid search over the hyperparameters of the model, such as: learning rate, batch size, dropout rate, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*/!\\ For nabeel: I am new to wandb, and I have some trouble giving some extra arguments to sweeps config (that are not used for training), which I think you successfully did in your notebook. Maybe you could give your sweep config here as an example (without all of the \"right\" answers for the hyperparameters so they can still work for it.)* They are hardcoded in the code underneath (metadata_cols)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to put all of the logic above in a function that we can call from the sweep configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config):\n",
    "    WandBLogger = wandb.keras.WandbMetricsLogger(log_freq=5)\n",
    "    loss = config['loss']\n",
    "    metadata_cols = [\n",
    "        \"sgscore1\",\n",
    "        \"distpsnr1\",\n",
    "        \"sgscore2\",\n",
    "        \"distpsnr2\",\n",
    "        \"fwhm\",\n",
    "        \"magpsf\",\n",
    "        \"sigmapsf\",\n",
    "        \"ra\",\n",
    "        \"dec\",\n",
    "        \"diffmaglim\",\n",
    "        \"ndethist\",\n",
    "        \"nmtchps\",\n",
    "        \"age\",\n",
    "        \"peakmag_so_far\"\n",
    "    ]\n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.keras.utils.set_random_seed(config[\"random_seed\"])\n",
    "    #TODO: temporary try except block to create fake data, to remove once we have the dataset.\n",
    "    try:\n",
    "        cand = pd.read_csv(f'data/train_cand_{config[\"train_data_version\"]}.csv')\n",
    "        triplets = np.load(f'data/train_triplets_{config[\"train_data_version\"]}.npy', mmap_mode='r')\n",
    "        val_cand = pd.read_csv(f'data/val_cand_{config[\"train_data_version\"]}.csv')\n",
    "        val_triplets = np.load(f'data/val_triplets_{config[\"train_data_version\"]}.npy', mmap_mode='r')\n",
    "    except FileNotFoundError:\n",
    "        cand = pd.DataFrame(np.zeros((100, 20)))\n",
    "        cand['objectId'] = np.random.randint(100, size=100).astype(str)\n",
    "        for col in metadata_cols:\n",
    "            cand[col] = np.random.rand(100)\n",
    "        cand['label'] = np.random.randint(2, size=100)\n",
    "        triplets = np.zeros((100, 63, 63, 3))\n",
    "        \n",
    "        val_cand = pd.DataFrame(np.zeros((100, 20)))\n",
    "        val_cand['objectId'] = np.random.randint(100, size=100).astype(str)\n",
    "        for col in metadata_cols:\n",
    "            val_cand[col] = np.random.rand(100)\n",
    "        val_cand['label'] = np.random.randint(2, size=100)\n",
    "        val_triplets = np.zeros((100, 63, 63, 3))\n",
    "\n",
    "    gen_cols = np.append(metadata_cols, ['label'])\n",
    "\n",
    "    x_train, y_train = triplets, cand['label']\n",
    "    x_val, y_val = val_triplets, val_cand['label']\n",
    "\n",
    "    train_df = cand[gen_cols]\n",
    "    val_df = val_cand[gen_cols]\n",
    "\n",
    "\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        verbose=1, \n",
    "        patience=config['early_stopping_patience']\n",
    "    )\n",
    "\n",
    "    LR_plateau = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", \n",
    "        patience=config['LR_plateau_patience'],\n",
    "        factor=config['reduce_LR_factor'],\n",
    "        min_lr=config['reduce_LR_minLR'],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    train_datagen = tf.keras.preprocessing.image.ImageDataGenerator()\n",
    "    val_datagen = tf.keras.preprocessing.image.ImageDataGenerator()\n",
    "\n",
    "    t_generator = train_datagen.flow(x_train, train_df, batch_size=config[\"batch_size\"], seed=config[\"random_seed\"], shuffle=False)\n",
    "    v_generator = val_datagen.flow(x_val, val_df, batch_size=config[\"batch_size\"], seed=config[\"random_seed\"], shuffle=False)\n",
    "\n",
    "    def multiinput_train_generator():\n",
    "        while True:\n",
    "            # get the data from the generator\n",
    "            # data is [[img], [metadata and labels]]\n",
    "            # yields batch_size number of entries\n",
    "            data = t_generator.next()\n",
    "\n",
    "            imgs = data[0]\n",
    "            cols = data[1][:,:-1]\n",
    "            targets = data[1][:,-1:]\n",
    "\n",
    "            yield [imgs, cols], targets\n",
    "\n",
    "    def multiinput_val_generator():\n",
    "        while True:\n",
    "            data = v_generator.next()\n",
    "\n",
    "            imgs = data[0]\n",
    "            cols = data[1][:,:-1]\n",
    "            targets = data[1][:,-1:]\n",
    "\n",
    "            yield [imgs, cols], targets\n",
    "\n",
    "    training_generator = multiinput_train_generator()\n",
    "    validation_generator = multiinput_val_generator()\n",
    "\n",
    "    model = BTSModel(config)\n",
    "    # you might want to replace tf.keras.optimizers.Adam with tf.keras.optimizers.legacy.Adam if you are using tensorflow v2.11+ on M1/M2/M3 macs\n",
    "    optimizer = tf.keras.optimizers.legacy.Adam(\n",
    "        learning_rate=config['learning_rate'], \n",
    "        beta_1=config['beta_1'],\n",
    "        beta_2=config['beta_2']\n",
    "    )\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(\n",
    "        training_generator,\n",
    "        steps_per_epoch=0.8*len(x_train) // config[\"batch_size\"],\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=(0.8*len(x_val)) // config[\"batch_size\"],\n",
    "        #class_weight=class_weight,\n",
    "        epochs=config[\"epochs\"],\n",
    "        verbose=1, callbacks=[early_stopping, LR_plateau, WandBLogger]\n",
    "    )\n",
    "\n",
    "    raw_preds = model.predict([triplets, cand.loc[:,metadata_cols]], batch_size=config['batch_size'], verbose=1)\n",
    "    preds = np.rint(np.transpose(raw_preds))[0].astype(int)\n",
    "    labels = cand[\"label\"].to_numpy(dtype=int)\n",
    "\n",
    "    results = preds == labels\n",
    "    print(f\"Overall validation accuracy {100*np.sum(results) / len(results):.2f}%\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick wrapper around the train function so it can be called nicely by a wandb sweep agent\n",
    "def sweep_train(config=None):\n",
    "    with wandb.init(config=config) as run:\n",
    "        train(run.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll start the gridsearch. No need to wait for it to finish before you can see what it is doing. You can just go to your sweep's overview page on wandb to see the progress of the gridsearch, and the best performing runs so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Before running the next cell, you need to create a sweep on wandb.ai. You can do so by clicking on the \"Sweeps\" tab, and then on \"Create new sweep\". You can use this sweep config:\n",
    "```yaml\n",
    "method: bayes\n",
    "metric:\n",
    "  goal: minimize\n",
    "  name: batch/loss\n",
    "parameters:\n",
    "  LR_plateau_patience:\n",
    "    distribution: int_uniform\n",
    "    max: 40\n",
    "    min: 10\n",
    "  batch_size:\n",
    "    distribution: int_uniform\n",
    "    max: 64\n",
    "    min: 16\n",
    "  beta_1:\n",
    "    distribution: uniform\n",
    "    max: 1.8\n",
    "    min: 0.45\n",
    "  beta_2:\n",
    "    distribution: uniform\n",
    "    max: 1.998\n",
    "    min: 0.4995\n",
    "  dropout_1:\n",
    "    distribution: uniform\n",
    "    max: 0.4\n",
    "    min: 0.15\n",
    "  dropout_2:\n",
    "    distribution: uniform\n",
    "    max: 0.4\n",
    "    min: 0.15\n",
    "  dropout_3:\n",
    "    distribution: uniform\n",
    "    max: 0.4\n",
    "    min: 0.15\n",
    "  early_stopping_patience:\n",
    "    distribution: int_uniform\n",
    "    max: 15\n",
    "    min: 5\n",
    "  epochs:\n",
    "    distribution: int_uniform\n",
    "    max: 20\n",
    "    min: 5\n",
    "  learning_rate:\n",
    "    distribution: uniform\n",
    "    max: 0.002\n",
    "    min: 0.0005\n",
    "  loss:\n",
    "    distribution: categorical\n",
    "    values:\n",
    "      - binary_crossentropy\n",
    "  optimizer:\n",
    "    distribution: categorical\n",
    "    values:\n",
    "      - adam\n",
    "  random_seed:\n",
    "    distribution: categorical\n",
    "    values: \n",
    "     - 42\n",
    "  reduce_LR_factor:\n",
    "    distribution: uniform\n",
    "    max: 1\n",
    "    min: 0.25\n",
    "  reduce_LR_minLR:\n",
    "    distribution: uniform\n",
    "    max: 2e-06\n",
    "    min: 5e-07\n",
    "  train_data_version:\n",
    "    distribution: categorical\n",
    "    values:\n",
    "      - ZTFSS\n",
    "  image_size:\n",
    "    distribution: categorical\n",
    "    values:\n",
    "      - (63,63,3)\n",
    "  metadata_size:\n",
    "    distribution: categorical\n",
    "    values:\n",
    "      - (14,)\n",
    "program: train.py\n",
    "```\n",
    "\n",
    "and replace the sweep_id in the cell below with the one you get from wandb.ai.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a sweep on WandB by going to the project page and clicking \"Sweep\"\n",
    "sweep_id = \"3f3ocp9c\" #replace with your sweep id here\n",
    "wandb.agent(sweep_id=sweep_id, function=sweep_train, count=10, project=\"BTSbot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*PS: You might see that often, the loss is `nan`. This is simply because the parameters used by that particular sweep yield very bad results, which get the model to diverge. This is why it's important to have a large number of runs in the sweep, to make sure we don't miss the best hyperparameters because of a few bad runs. But also, create new sweeps with tighter constraints for some of the hyperparameters.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "##### We'd like you to try and beat the best score we've got so far! \n",
    "\n",
    "Here's what you can do, in order of difficulty (from easiest to hardest):\n",
    "1. Create a new clean sweep on wandb.ai, with the configuration of your choice, adding, modifying, or removing hyperparameters as you wish. You might want to increase the worker count to cover more of the parameter space.\n",
    "\n",
    "2. Try different loss functions and optimizers, and improve on how the initial weights are initialized.\n",
    "\n",
    "3. Perform data augmentation on the cutouts, and use the augmented data to train the model. You can use the tf.keras.preprocessing.image.ImageDataGenerator (which you'll find in the code above already) to do this. You could also do the same for the metadata, but it's a little more complicated.\n",
    "\n",
    "4. Play with the model's architecture. You can add, remove, or modify the layers as you wish. You could also try to use a different CNN architecture, such as ResNet, EfficientNet, etc.\n",
    "\n",
    "5. Change the features used in the metadata model. You can add, remove, or modify the features used. You could also compute new features from the existing ones.\n",
    "\n",
    "6. Improve time needed for inference. You can try to reduce the number of parameters in the model, or use a more efficient CNN architecture to make it more efficient. This is *key* for the model to be used in production, as we expect it to run on thousands of alerts every day.\n",
    "\n",
    "7. Be creative! There are an infinite number of ways to improve a model, and we're sure you can come up with something we haven't thought of :)\n",
    "\n",
    "**Once you have a model you are happy with, you should use the code you wrote in the [Testing the model](#Testing-the-model) section to evaluate it on the test set, and plot some of the metrics.**\n",
    "\n",
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
