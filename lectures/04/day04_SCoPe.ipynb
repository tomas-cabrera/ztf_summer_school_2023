{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270db8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scope.utils import read_parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16bcf64",
   "metadata": {},
   "source": [
    "# Data on Google Drive\n",
    "#### https://drive.google.com/drive/folders/13cm3Tf3RtudlVA5fBaMcHVi8anOnn8Gy?usp=sharing\n",
    "\n",
    "***Available data include:***\n",
    "- a field of generated SCoPe features from ZTF DR16 (`field_297`)\n",
    "- text files with current SCoPe model performance\n",
    "- the full training set, including features generated from DR16\n",
    "- a downsampled training set containing 10% of the full training set's rows\n",
    "- a column guide describing selected columns of the training sets\n",
    "- DR16 light curves associated with the downsampled training set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76978085",
   "metadata": {},
   "source": [
    "## Outline of Notebook:\n",
    "- **Installing SCoPe**\n",
    "- **Training**\n",
    "- **Plotting classifier performance**\n",
    "- **Inference**\n",
    "- **Examining predictions**\n",
    "- **Plot Field 297 predictions**\n",
    "- **Train a DNN classifier**\n",
    "\n",
    "**Tasks** are intended to be the primary means of interacting with this notebook.\n",
    "\n",
    "***Notes*** are meant to more broadly describe SCoPe functionality, but you're welcome to explore these avenues further if time permits. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e18bbb4",
   "metadata": {},
   "source": [
    "# Installing SCoPe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1611d1d0",
   "metadata": {},
   "source": [
    "The SCoPe GitHub repository is located here: https://github.com/ZwickyTransientFacility/scope\n",
    "\n",
    "Follow the instructions here to install SCoPe on your computer: https://zwickytransientfacility.github.io/scope-docs/developer.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6070c568",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfab0035",
   "metadata": {},
   "source": [
    " By default, SCoPe code is run on the command line. We use `os.system` to call `scope.py train` via this notebook.\n",
    " \n",
    "**Tasks:**\n",
    " \n",
    "- To start, open the `SCoPe training set column guide` on Google Drive to have a reference for the columns of the training set.\n",
    "\n",
    "- Download the `DR16_merged_classifications_features_revamped_updated_imputed.parquet` training set from Google Drive, placing it within your `scope` directory.\n",
    " \n",
    "- Change the dataset path in `config.yaml` before running the following code. The path specifies the location within the `scope` directory where you put the training set. In `config.yaml`, the path can be found under `training: dataset: `."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db60f1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We begin by specifying the tag (or label) on which to train a binary classifier:\n",
    "tag = 'vnv'\n",
    "# See all tags under \"Labels\" in the \"SCoPe training set column guide\" Google Sheet\n",
    "\n",
    "# SCoPe supports neural network (dnn) and XGBoost (xgb) algorithms:\n",
    "algorithm = 'xgb'\n",
    "\n",
    "# If --save is passed, training results are saved to the group named below: \n",
    "group = 'ss23'\n",
    "\n",
    "# SCoPe determines light curve periods using GPU-accelerated algorithms.\n",
    "# These algorithms include a Lomb-Scargle approach (ELS), Conditional Entropy (ECE),\n",
    "# Analysis of Variance (AOV), and an approach nesting all three (ELS_ECE_EAOV).\n",
    "# Periodic features are stored with the suffix specified below:\n",
    "period_suffix = 'ELS_ECE_EAOV'\n",
    "\n",
    "# We require at least min_count positive examples to run training.\n",
    "min_count = 1000\n",
    "\n",
    "# Neural network training takes an --epochs argument that we set to 30 here.\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bd66f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(pathlib.Path.home() / f'scope/scope.py train {tag} --algorithm={algorithm} \\\n",
    "          --group={group} --period_suffix={period_suffix} --epochs={epochs} --verbose --save --plot --skip_cv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b06efce",
   "metadata": {},
   "source": [
    "***Notes:***\n",
    "\n",
    "*The above training runs the XGB algorithm by default and skips cross-validation in the interest of time. If you have time after going through the notebook, you can remove the `--skip_cv` argument to run a cross-validated grid search of XGB hyperparameters during training.*\n",
    "\n",
    "*DNN hyperparameters have already been optimized using a different approach - Weights and Biases Sweeps (https://docs.wandb.ai/guides/sweeps). The results of these sweeps have been saved in the config file. To run another round of sweeps for DNN, you can create a WandB account and set the `--run_sweeps` keyword in your call to `scope.py train`.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c302c0e",
   "metadata": {},
   "source": [
    "**Task: train multiple classifiers with one script**\n",
    "- Run the cell below to use `scope.py create_training_script` to generate a script that trains many classifiers sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9b1733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: you will get an error if you try to create an inference script with a name that already exists\n",
    "os.system(pathlib.Path.home() / f'scope/scope.py create_training_script --filename=train_{algorithm}_ss.sh\\\n",
    "          --min_count={min_count} --algorithm={algorithm} --period_suffix={period_suffix} --add_keywords=\"--save --plot --group={group} --epochs={epochs} --skip_cv\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a4535f",
   "metadata": {},
   "source": [
    "- Update script permissions, adding executable permissions to the new training script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7766a49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(f'chmod +x $HOME/scope/train_{algorithm}_ss.sh')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315d9783",
   "metadata": {},
   "source": [
    "- Run the training script you generated in a terminal window (using `./train_xgb_ss.sh`) to train multiple labels for the XGB algorithm. This could take ~15-20 minutes to finish for all classifiers. Continue to the \"Plotting classifier performance\" section in the meantime."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54be7b7a",
   "metadata": {},
   "source": [
    "***Note: running training on HPC resources***\n",
    "\n",
    "*`train_algorithm_slurm.py` and `train_algorithm_job_submission.py` can be used generate and submit `slurm` scripts to train all classifiers in parallel using HPC resources.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec5b309",
   "metadata": {},
   "source": [
    "# Plotting classifier performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55cf115",
   "metadata": {},
   "source": [
    "SCoPe saves diagnostic plots and json files to report each classifier's performance. The below code shows the location of these results for one classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456d2513",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model = pathlib.Path.home() / f'scope/models_{algorithm}/{group}/{tag}'\n",
    "path_stats = [x for x in path_model.glob('*plots/val/*stats.json')][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557a9270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to model\n",
    "print(path_model)\n",
    "\n",
    "# Path to performance stats json (validation set)\n",
    "print(path_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915d0764",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_stats) as f:\n",
    "    stats = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7906a8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ab75b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plt.rcParams['font.size']=13\n",
    "plt.title(f\"{algorithm} performance ({tag})\")\n",
    "plt.bar(tag, stats['precision'], color='blue',width=1,label='precision')\n",
    "plt.bar(tag, stats['recall'], color='red',width=0.6, label='recall')\n",
    "plt.legend(ncol=2,loc=0)\n",
    "plt.ylim(0,1.15)\n",
    "plt.xlim(-3,3)\n",
    "plt.ylabel('Score')\n",
    "\n",
    "# Can also loop over many labels to compare each one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db58f617",
   "metadata": {},
   "source": [
    "**Tasks:**\n",
    "- In `scope/models_xgb/ss23`, find and examine the `vnv` diagnostic plots/text files for the validation set (`val`).\n",
    "    - How are the accuracy, precision, recall, F1 score, and area under the ROC curve defined?\n",
    "    - What are the top three most important features for the `vnv` classifier? Bottom three? (Use results files with `impvars` in the name)\n",
    "- As more models complete training, write code to plot the results for each classifier in a way that compares their validation precision/recall. **Once training completes for all models, continue on to the \"Inference\" section and resume plotting work after you complete that section.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f828d456",
   "metadata": {},
   "source": [
    "#### Define performance stats here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01af9652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature importance here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8308ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot classifier results here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28a7888",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f6c14e",
   "metadata": {},
   "source": [
    "We use `tools/inference.py` to run inference on a field of generated features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d827bb4d",
   "metadata": {},
   "source": [
    "**Tasks:**\n",
    "- Download the `generated_features` directory from Google Drive and place it within your `scope` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7348dd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_directory = 'generated_features'\n",
    "field = 297"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7a9163",
   "metadata": {},
   "source": [
    "- Generate an inference script. This is the easiest way to run inference with all trained models.\n",
    "### Ensure that the training script is finished before running the below cell: ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547aa94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note you will get an error if you try to create an inference script with a name that already exists\n",
    "os.system(pathlib.Path.home() / f'scope/scope.py create_inference_script --filename=get_all_preds_ss_{algorithm}.sh --group_name={group} \\\n",
    "          --algorithm={algorithm} --period_suffix={period_suffix} --feature_directory={feature_directory}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60074165",
   "metadata": {},
   "source": [
    "- Add executable permissions to the new inference script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9ea59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(f'chmod +x $HOME/scope/get_all_preds_ss_{algorithm}.sh')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ac93d4",
   "metadata": {},
   "source": [
    "- Run inference by calling the inference script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94de6856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can take a few minutes to impute features before displaying output\n",
    "os.system(pathlib.Path.home() / f'scope/get_all_preds_ss_{algorithm}.sh {field}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d821a672",
   "metadata": {},
   "source": [
    "***Note: running inference on HPC resources***\n",
    "\n",
    "*`run_inference_slurm.py` and `run_inference_job_submission.py` can be used generate and submit `slurm` scripts to run inference for all classifiers in parallel using HPC resources.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646d240a",
   "metadata": {},
   "source": [
    "# Examining predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659e34ec",
   "metadata": {},
   "source": [
    "The result of running the inference script will be a parquet file containing some descriptive columns followed by columns containing for each classification's probability for each source in the field. By default, the file is located as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a755db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_preds = pathlib.Path.home() / f\"scope/preds_{algorithm}/field_{field}/field_{field}.parquet\"\n",
    "print(path_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4270b7",
   "metadata": {},
   "source": [
    "**Tasks:**\n",
    "- Use SCoPe's `read_parquet` utility to read the predictions file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045f1c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = read_parquet(path_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e51e29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63944276",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2d1c12",
   "metadata": {},
   "source": [
    "# Plot Field 297 predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59ecf4d",
   "metadata": {},
   "source": [
    "**Tasks:**\n",
    "- Make a histogram of probabilities for a single classification in Field 297.\n",
    "- Make a scatter plot comparing the probabilities of two related classifications (e.g. `vnv` and `rrlyr`).\n",
    "- Determine what fraction of Field 297 sources have a `vnv` probability greater than 0.7.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61f2f60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86e0755a",
   "metadata": {},
   "source": [
    "# Train a DNN classifier\n",
    "\n",
    "**Tasks:**\n",
    "- Once finished with these tasks, return to the top of the notebook and choose one classification to rerun training/inference for DNN, setting `algorithm = 'dnn'`. You can choose more than one, but it will take longer. The defaults in this notebook should take ~15 mins to train one DNN classifier.\n",
    "- Compare training performance between the XGB and DNN classifiers. Which algorithm performs better for your chosen classification?\n",
    "- Compare predictions between the XGB and DNN algorithms for your chosen classification. What differences do you see?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e730345",
   "metadata": {},
   "source": [
    "***Note:***\n",
    "*SCoPe DNN training does not provide feature importance information (due to the hidden layers of the network). Feature importance is possible to estimate for neural networks, but it is more computationally expensive compared to this \"free\" information from XGB.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d413add1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
